{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Exercise 08\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "Herein we will be introducing and discussing some machine learning models at a high level to get first time and beginner biomedical informaticians a taste for some of the problems machine learning can help solve. Furthermore, we will implement both a logisitic regression model, a vanilla neural network, decision tree, and random foret model and evaluate the efficacy of these models on an example dataset.\n",
    "\n",
    "As this is a brief survey course on machine learning, we will not dive too deep into the theory of these models, nor will we be able to cover every type of model. At the end of this notebook I have provided a list of references and resources that biomedical informaticians, beginner, novice, and experts alike, may find useful and insightful.\n",
    "\n",
    "<img width=\"800px\" src=\"img/extended_ml_cheat_sheet.jpeg\"/>\n",
    "\n",
    "[An Extended Version Of The Scikit-Learn Cheat Sheet](https://medium.com/@chris_bour/an-extended-version-of-the-scikit-learn-cheat-sheet-5f46efc6cbb)\n",
    "\n",
    "**Disclaimer:** Understanding the dataset with which you are working, ensuring it is clean, workable data, and formulating hypotheses you wish to test with your data are all ***VERY*** important aspects to consider before diving into machine or deep learning models with your data. **The above figure, and corresponding article, depict this point excellently.**\n",
    "\n",
    "https://scikit-learn.org/stable/common_pitfalls.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of machine learning\n",
    "\n",
    "There are many different types of models in machine learning and choosing the best one is dependent on:\n",
    "1. The problem you aim to solve\n",
    "2. The data you have\n",
    "\n",
    "In some instances multiple models may work well for you, in which case you will have to consider other aspects of the model, such as:\n",
    "* interpretability\n",
    "* memory cost\n",
    "* number of samples\n",
    "* dimensionality\n",
    "* and so on...\n",
    "\n",
    "Though these considerations may help you narrow down your choices, choosing the *best* remains a difficult task. I will provide some general information about different types of machine learning models while keeping some of the above aspects in mind.\n",
    "\n",
    "Below is a figure that shows a very well defined hierarchy of different ML models that one can consider. The upper level of this hierarchy gives 3 main learning types: **supervised**, **unsupervised**, and **reinforcement**. I will discuss all 3 of these as well as a fourth, called **semi-supervised**.\n",
    "\n",
    "<img width=\"500px\" src=\"img/ml_hierarchy.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "Samples of input-output pairs (labelled outcomes)\n",
    "\n",
    "**Classification** - predict the binary (or class) label for an unlabeled sample. Examples: logistic regression, SVM\n",
    "\n",
    "**Regression** - predict a real-valued label for an unlabeled sample. Examples: linear regression \n",
    "\n",
    "<img width=\"400px\" src=\"img/class_v_reg.png\"/>\n",
    "\n",
    "In classification models, the boundary separating the examples of different classes is called the *decision boundary*. For regression models, the line that best fits the data is the *regression line*. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Supervised_learning\n",
    "\n",
    "https://scikit-learn.org/stable/supervised_learning.html\n",
    "\n",
    "*Logistic regression and the vanilla neural network we will implement herein are both considered supervised classification models. Therefore, this model type will be our main focus for this notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning\n",
    "\n",
    "Draw inferences and patterns from input data without labelled output data. Examples: k-means, PCA\n",
    "\n",
    "<img width=\"400px\" src=\"img/clusters.png\"/>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Unsupervised_learning\n",
    "\n",
    "https://scikit-learn.org/stable/unsupervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised learning\n",
    "\n",
    "Supervised learning tasks and techniques that also make use of unlabeled data for training.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Semi-supervised_learning\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.semi_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning\n",
    "\n",
    "Model tries a bunch of different things and is given a reward signal when doing something well.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Reinforcement_learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance tradeoff\n",
    "\n",
    "**Variance** is the error in the model due to sensitivity to the data. \n",
    "\n",
    "High variance means our model is *overfit* and has been trained to the noise in the data.\n",
    "\n",
    "**Bias** is the error between the predicted and known labels for our data.\n",
    "\n",
    "High bias means our model is *underfit* and does not predict the correct outcome for our data.\n",
    "\n",
    "<img width=\"400px\" src=\"img/hl_bias-hl_variance.png\" /> \n",
    "\n",
    "We can think about (A) high variance and high bias, (B) low variance and high bias, (C) high variance and low bias, and (D) low variance and low bias. We are aiming to accomplish (D)!\n",
    "\n",
    "So, when we look at the plot below we can relate variance and bias directly to how we are training our models. We are looking to hit that sweet spot where the bias and variance curves intersect. That is a good model.\n",
    "\n",
    "<img width=\"400px\" src=\"img/variance_v_bias.jpeg\" /> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Logistic regression is a simple and commonly used machine learning algorithm for two-class classification. It is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. Logistic regression can be used to answer questions such as:\n",
    "* How does the probability of getting lung cancer (yes vs. no) change for every additional pound a person is overweight and for every pack of cigarettes smoked per day?\n",
    "* Do body weight, calorie intake, fat intake, and age have an influence on the probability of having a heart attack (yes vs. no)?\n",
    "\n",
    "*Logistic regression can also be used for multi-class predictions, but we will not cover that here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, logistic regression uses a linear combination of more than one feature value or explanatory variable as argument of the sigmoid function:\n",
    "\n",
    "$f(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "The corresponding output of the sigmoid function is a number between 0 and 1. \n",
    "\n",
    "<img width=\"300px\" src=\"img/sigmoid.png\"/>\n",
    "\n",
    "The middle value is considered as threshold to establish what belongs to the class 1 and to the class 0. In particular, an input producing an outcome greater than 0.5 is considered belonging to the class 1. Conversely, if the output is less than 0.5, then the corresponding input is classified as belonging to class 0.\n",
    "\n",
    "For our logistic regression model we use the logistic function:\n",
    "\n",
    "$f_{w,b}(x) = \\frac{1}{1+e^{-(wx+b)}}$\n",
    "\n",
    "The logistic function is our **activation function**. This is going to tell us when a sample is 0 or 1.\n",
    "\n",
    "To calculate the solution to this equation, i.e. obtain the best intercept and coefficients, we aim to maximize the **log likelihood** of the training data.\n",
    "\n",
    "$ \\ln L_{\\mathbf{w},b} = \\sum_{i=1}^{N}y_{i}\\ln f_{\\mathbf{w},b}(x_{i})+(1-{y_{i}}) \\ln (1-f_{\\textbf{w},b}(x_{i})$\n",
    "\n",
    "Though it may appear daunting, when you break it down, it isn't that bad. When $y_{i}=1$, the second part of the summation drops out (1-1=0), whereas when $y_{i}=0$ the first part of the equation drops out. \n",
    "\n",
    "Log likelihood is our **cost function**. Generally, minimizing functions is preferred over maximizing, so the negative of the function is commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "The general point is there are many methods that have been developed and well tested for optimizing (maximizing or minimizing) a function. You start with a guess then you adjust the parameters over several iterations until you have converged to some point (number of iterations, tolerance, etc.).\n",
    "\n",
    "<img width=\"300px\" src=\"img/minimization.gif\"/>\n",
    "\n",
    "\n",
    "I do not go into specific optimizers here, for the sake of time. I do think I will update this notebook in the future to include a nice section on optimizers.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Gradient_descent\n",
    "\n",
    "https://scikit-learn.org/stable/modules/sgd.html\n",
    "\n",
    "https://en.wikipedia.org/wiki/Limited-memory_BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our implementation of logistic regression, we will use scikit-learn's LogisticRegression model:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Let's load the libraries we will be using..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pds\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, \\\n",
    "    f1_score, roc_auc_score, auc, precision_recall_curve, roc_curve,\\\n",
    "    classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load Pima Indians Diabetes dataset (downloaded May 14, 2019; N=768)\n",
    "df = pds.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missingness\n",
    "\n",
    "The only cleaning we need to do is to drop the rows that contain missing values. In general practice, you do not remove these rows without further exploratory analysis. However, for sake of this example, we have omitted rows that contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to determine of a row has an missing value\n",
    "def valid_value(row):\n",
    "    if 0 == row['Glucose'] or \\\n",
    "       0 == row['BloodPressure'] or \\\n",
    "       0 == row['SkinThickness'] or \\\n",
    "       0 == row['Insulin'] or \\\n",
    "       0 == row['BMI'] or \\\n",
    "       0 == row['Age']:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "## create dataframe with only valid rows\n",
    "df_pima = df[df.apply(lambda row: valid_value(row), axis=1)]\n",
    "df_pima.dropna(inplace=True)\n",
    "df_pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"length of original dataframe: {len(df)}\")\n",
    "print(f\"length of filtered dataframe: {len(df_pima)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting samples\n",
    "\n",
    "Now we split the data into our training and test sets. We do this because we need to train the model on some of the data and ensure that we have generalizable model by testing the optimized model on samples it has never seen.\n",
    "\n",
    "<img width=\"600px\" src=\"img/data.png\" />\n",
    "\n",
    "First, we separate the features. By convention, scikit-learn often refers to the feature dataset as `X` and the target dataset as `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset in features and target variable\n",
    "feature_cols = \\\n",
    "    ['Pregnancies', 'Insulin', 'BMI', 'Age','Glucose',\n",
    "     'BloodPressure','DiabetesPedigreeFunction', 'SkinThickness']\n",
    "\n",
    "X = df_pima[feature_cols]\n",
    "y = df_pima['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.3, random_state=42) # 70% training and 30% test\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the test and training data is fit to our model and we predict outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a logistic regression classifier and predict\n",
    "model = LogisticRegression(random_state=42, max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:,1]\n",
    "print(f\"Our model converged after {model.n_iter_[0]} iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "To evaluate our logistic regression model, we will examine the confusion matrix, accuracy score, precision score, recall score, and f1 score. \n",
    "\n",
    "**Accuracy:**\n",
    "* $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "* Accuracy is the ratio of correct predictions to total predictions made. However, there are problems with accuracy. It assumes equal costs for both kinds of errors. A 99% accuracy can be excellent, good, mediocre, poor or terrible depending upon the problem.\n",
    "\n",
    "**Precision:**\n",
    "* $\\frac{TP}{TP + FP}$\n",
    "* Precision is the ability of a classifier not to label an instance positive that is actually negative. High precision indicates a small number of false positives.\n",
    "\n",
    "**Recall:**\n",
    "* $\\frac{TP}{TP + FN}$\n",
    "* Recall is the ability of a classifier to find all positive instances. High recall indicates a small number of false negatives.\n",
    "\n",
    "<img width=\"300px\" src=\"img/precision-recall.png\" />\n",
    "\n",
    "**F1 score (F measure):**\n",
    "* $\\frac{2 * Recall * Precision}{Recall + Precision}$\n",
    "* Since we have two measures (Precision and Recall) it helps to have a measurement that represents both of them. We calculate an F1 score that uses Harmonic Mean in place of Arithmetic Mean as it punishes the extreme values more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(y_test, y_pred, palette=\"inferno\"):\n",
    "    ## see: https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "    ##      https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n",
    "    ##      https://classeval.wordpress.com/introduction/basic-evaluation-measures/\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    colors = sns.color_palette(palette) # set the colors to use for heatmap\n",
    "    # print(colors.as_hex()) # uncomment this to see color palette\n",
    "\n",
    "    ax = sns.heatmap(matrix, square=True, annot=True, fmt='d', \n",
    "                     cbar=False, cmap=colors, vmin=-1, annot_kws={\"size\":13}, linewidths=1.0)\n",
    "\n",
    "    # set labels on figure\n",
    "    ax.set_xticklabels(labels=[\"neg\",\"pos\"], fontsize=13)\n",
    "    ax.set_yticklabels(labels=[\"neg\",\"pos\"], fontsize= 13)\n",
    "    plt.xlabel(\"\\nactual value\", fontsize=15)\n",
    "    plt.ylabel(\"predicted value\\n\", fontsize=15)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_static_roc_curve(fpr, tpr):\n",
    "    plt.figure(figsize=[5,5])\n",
    "    plt.fill_between(fpr, tpr, alpha=.5, color='darkorange')\n",
    "    # Add dashed line with a slope of 1\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2)\n",
    "    plt.plot([0,1], [0,1], linestyle=(0, (5, 5)), linewidth=2)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC curve\");\n",
    "    \n",
    "def plot_static_pr_curve(recall, precision):\n",
    "    plt.figure(figsize=[5,5])\n",
    "    plt.fill_between(recall, precision, alpha=.5, color='darkorange')\n",
    "    plt.plot(recall, precision, color='darkorange', lw=2)\n",
    "    # Add dashed line with a slope of 1\n",
    "    plt.plot([1,0], [0,1], linestyle=(0, (5, 5)), linewidth=2)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-recall curve\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show confustion matrix\n",
    "show_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver operating characteristic curve\n",
    "\n",
    "Another common metric for classification problems is area under the receiver operating characteristic (ROC) curve. This plot, and associated scalar metric, assesses a model's performance by comparing the true positive rate (TPR) to the false positive rate (FPR) at varying thresholds. What this means is as the threshold varies from 0 to 1, can the model still successfully discern the two classes.\n",
    "\n",
    "**Sensitivity/True positive rate (TPR):**\n",
    "* TPR = TP / (TP + FN)\n",
    "* TPR is the ability of a classifier to find all positive instances. High TPR indicates a small number of false negatives.\n",
    "\n",
    "**Specificity/True negative rate (TNR):**\n",
    "* TNR = TN / (TN + FP)\n",
    "* TNR is the ability of a classifier to find all negative instances. High TNR indicates a small number of false positives.\n",
    "\n",
    "**False positive rate (FPR):**\n",
    "* FPR = FP / (FP + TN)\n",
    "* FPR is the ability of a classifier to incorrectly predicting positives instances. Low FPR indications a small number of false positives.\n",
    "\n",
    "<img width=\"300px\" src=\"img/sensitivity-specificity.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "plot_static_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision recall curve\n",
    "\n",
    "Related to the above ROC curve, the precision recall curve, and the area under it, is often used as a metric to determine classification model efficacy. This metric is especially important for imbalanced class sizes -- one class has far more samples (majority) than the other class (minority). The difference between this curve and the ROC curve is the use of precision instead of FPR (TPR and recall are the same metric). Precision is less affected by a large number of negative samples because it takes into account true positives and false positives. On the other hand, FPR is based upon false positives and true negatives so for a large number of negative samples this metric would very slowly increase. **Precision is focused on the positive class**\n",
    "\n",
    "*Use AUPR when you have imbalanced classes, specifically when you have a majority of negative samples (usually the case).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_static_pr_curve(recall,precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting artifact of this plot, the rapid drop to the left of the plot is due to the way the curve is calculated. As you move right to left on the plot the threshold for determining if a prediction is postive becomes more stringent. So on the far left, there are very few positive class predictions (both true and false positives), so adding one more false or true positive (moving towards the right) can greatly affect the precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability and feature importance\n",
    "\n",
    "We will analyze the model a bit more to understand what the model is doing and what features are most important to the classification.\n",
    "\n",
    "First, we will extract the coefficients from the model and match them with the corresponding name of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coefs = list(zip(feature_cols,model.coef_[0]))\n",
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we take the exponential of the coefficients to calculate the odds ratio for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "odds_ratio = [(x[0],math.exp(x[1])) for x in coefs]\n",
    "odds_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that of these features the most important appears to be Diabetes Pedigree Function. The odds ratio tells us that for every 1 unit increase in Diabetes Pedigree Function a patient is 2.53x more likely to experience the outcome (diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(x[0],(x[1]-1)*100.0) for x in odds_ratio]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the splitting\n",
    "\n",
    "What happens if we change the training/testing split to 80/20..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset into training set and test set\n",
    "X2_train, X2_test, y2_train, y2_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=42) # 80% training and 20% test\n",
    "\n",
    "## create a logistic regression classifier and predict\n",
    "model = LogisticRegression(random_state=42, max_iter=500)\n",
    "model.fit(X2_train, y2_train)\n",
    "y2_pred = model.predict(X2_test)\n",
    "y2_proba = model.predict_proba(X2_test)[:,1]\n",
    "print(f\"Our model converged after {model.n_iter_[0]} iterations.\")\n",
    "\n",
    "print(classification_report(y2_test, y2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Now that we understand and have run a logistic regression model, let's go a bit \"deeper\". We can think of a neural network (NN) as a set of nested functions -- we call these layers. Each layer in our model takes input from the previous layer and outputs directly to the next layer, i.e. fully connected. \n",
    "\n",
    "We are going to create a 3 layer neural network with the previously used 8 variables as features and the \"Outcome\" as the label. \n",
    "\n",
    "The first layer of our NN will take in all 8 features as input, has a ReLU (rectified linear unit) activation function, and outputs 12 latent features (hidden). As opposed to the logistic function, discussed previously, ReLU sets the input to 0 if it is <0 or uses the input as is if >0.\n",
    "\n",
    "$f(x)=max(0,x)$\n",
    "\n",
    "The second layer of our NN will take in all 12 latent features from the previous layer as input, has a ReLU (rectified linear unit) activation function, and outputs 8 latent features.\n",
    "\n",
    "The third (and last) layer of our model is a sigmoid output layer that takes in the previous 8 latent features as input.\n",
    "\n",
    "The loss function we use for this model is binary cross entropy, which basically sums the log probabilty of a given sample being in the 0 class and the log probability of the sample being in the 1 class across all samples. This is essentially the same function as the log likelihood. We want to minimize this loss function.\n",
    "\n",
    "$ \\ln Loss = \\sum_{i=1}^{N}-(y_{i}\\ln f(x_{i})+(1-{y_{i}}) \\ln (1-f(x_{i}))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our implementation of neural network, we will use keras's sequential model:\n",
    "* https://keras.io/guides/sequential_model/\n",
    "\n",
    "Let's load the libraries we will be using..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(42)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=75, batch_size=10)\n",
    "\n",
    "# make class predictions with the model\n",
    "y_proba = model.predict(X_test)\n",
    "y_pred = (y_proba > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## show confustion matrix\n",
    "show_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate F1 score\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate AUROC\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "plot_static_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate PR curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "# calculate AUPR\n",
    "auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_static_pr_curve(recall,precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting and Overfitting\n",
    "\n",
    "The below figure is an excellent example of underfitting, a good fit, and overfitting.\n",
    "\n",
    "<img width=\"800px\" src=\"img/model_fit.png\"/>\n",
    "\n",
    "There are methods for avoiding overfitting, such as regularization, dropout, etc. You can learn more about these in many of the references provided.\n",
    "\n",
    "https://scikit-learn.org/stable/model_selection.html\n",
    "\n",
    "*Choosing the right model(s), activation function(s), and **hyperparameters** are crucial for creating a robust and **generalizable** model.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "<img width=\"300px\" src=\"img/decision_tree-example.jpeg\" /> \n",
    "\n",
    "Decision trees can be used for both classification and regression.  They are similar to if/then statements.\n",
    "\n",
    "* Tree depth: how many questions do we ask until we reach our decision? (denoted by its longest route)\n",
    "* Root node: first decision\n",
    "* Leaf node: final node of the tree\n",
    "\n",
    "<img \n",
    "  width=\"500px\"\n",
    "  src=\"img/Decision-tree-showing-the-risk-of-malignant-effusion-when-medical-history-and-diagnostic.png\" /> \n",
    "\n",
    "How is a decision tree trained? The goal is finding the best feature that maximizes the information gain at each node. \n",
    "\n",
    "Entropy: Quantifies the amount of uncertainty associated with a specific probability distribution. The higher the entropy, the less confident we are in the outcome.\n",
    "\n",
    "Information Gain: How much do we gain (in terms of reduction in entropy) from knowing one of the features.\n",
    "\n",
    "Advantages: \n",
    "* easy to interpret\n",
    "* can use both qualitative and quantitative predictors and responses\n",
    "* reproducible in clinical workflow\n",
    "* fast and perform well on large datasets\n",
    "\n",
    "Disadvantages:\n",
    "* need an optimal choice at each node; at each step, the algorithm chooses the best result. Choosing the best result at a given step does not ensure an optimal decision when you make it to the leaf node\n",
    "* prone to over-fitting, especially with deep trees (fix: can set a max depth--this limits variance, but at the expense of bias!)\n",
    "\n",
    "Here are some links with more information about decision trees:\n",
    "* https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "* http://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/\n",
    "* https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pds\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pydotplus\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz # Import Decision Tree Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,\\\n",
    "        roc_auc_score, auc, precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn import tree\n",
    "from IPython.display import Image \n",
    "\n",
    "import random\n",
    "## set seed for randomization\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load Pima Indians Diabetes dataset (downloaded May 14, 2019; N=768)\n",
    "df = pds.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to determine of a row has an missing value\n",
    "def valid_value(row):\n",
    "    if 0 == row['Glucose'] or \\\n",
    "       0 == row['BloodPressure'] or \\\n",
    "       0 == row['SkinThickness'] or \\\n",
    "       0 == row['Insulin'] or \\\n",
    "       0 == row['BMI'] or \\\n",
    "       0 == row['Age']:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "## create dataframe with only valid rows\n",
    "df_pima = df[df.apply(lambda row: valid_value(row), axis=1)]\n",
    "df_pima.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing decision trees\n",
    "Now that the data has been inspected and cleaned, we can implement a decisio tree.\n",
    "\n",
    "For this will will use scikit-learn's `DecisionTreeClassifier()`:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "To use this funcition, we need to define a dataset that contains the features used to build our decision tree, and a dataset that contains the target (or outcome) we are trying to coming to arrive at decision about.\n",
    "\n",
    "By convention, scikit-learn often refers to the feature dataset as `X` and the target dataset as `y`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "feature_cols = \\\n",
    "    ['Pregnancies', 'Insulin', 'BMI', 'Age','Glucose',\n",
    "     'BloodPressure','DiabetesPedigreeFunction', 'SkinThickness']\n",
    "\n",
    "X = df_pima[feature_cols]\n",
    "y = df_pima['Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build the decision tree, we will need to split the dataset into a training set and a test set. The training set that is used to build the tree, and the test set that is used to evaluate it.\n",
    "\n",
    "This is necessary for both the feature dataset (`X`) and the target/outcome dataset (`y`).\n",
    "\n",
    "Scikit-learn's `train_test_split()` funciton allows us to easily do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.3, random_state=42) # 70% training and 30% test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the training and test datasets, we call the `DecisionTreeClassifier()` function  to build the decision tree. \n",
    "\n",
    "The hierarchical structure of a decision tree leads us to the final outcome by traversing through the nodes of the tree. Each node consists of an attribute or feature which is further split into more nodes as we move down the tree.\n",
    "\n",
    "The Gini index (or Gini impurity) is the default method used determine the features to used at each node. Briefly, Gini index measures the degree or probability of a particular variable being wrongly classified when it is randomly chosen. If all the elements belong to a single class, then it can be called pure. The degree of Gini index varies between 0 and 1, where 0 denotes that all elements belong to a certain class or if there exists only one class, and 1 denotes that the elements are randomly distributed across various classes. A Gini Index of 0.5 denotes equally distributed elements into some classes.\n",
    "\n",
    "*The goal is to have the lowest possible Gini index at each split.*\n",
    "\n",
    "More information about the Gini index and other methods for feature splitting is found here:\n",
    "* https://blog.quantinsti.com/gini-index/\n",
    "* https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code, I create a decision tree named `dtree`. After the tree is created, the `fit` method is called to train (i.e., build) the tree using the traing data `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decision tree classifer object\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "# train decision tree classifer\n",
    "dtree = dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the tree\n",
    "With the decision tree built, we can visualize the nodes. For this will use scikit-learn's `export_graphviz()` funciton:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "\n",
    "The generated figure is difficult read, but if you **squint** you may be able to make out the features and values used to make the splits. Importantly, the Gini index of all leaf nodes is `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from six import StringIO \n",
    "from IPython.display import Image, display\n",
    "\n",
    "# export the tree as dot\n",
    "dot_data = tree.export_graphviz(dtree, out_file=None, filled=True, \n",
    "                                feature_names=feature_cols, \n",
    "                                class_names=['neg','pos'])\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree pruning\n",
    "A disadvantage of decision trees is that they are prone to overfitting as they grow larger and more complex. In order to avoid or correct overfitting we can **prune** the tree. One way to do this is by limiting the maximum depth of the tree. Another way is use cost complexity pruning. The former just limits the length of the longest path of the decision tree, which will stop the tree from becoming too complex and overfitting the data. Cost complexity pruning places a penalty on the number of terminal leaves that exist in the model, thus restricting the complexity of the tree. These pruning methods may increase training error, but the resulting model will be far more robust to your testing data.\n",
    "\n",
    "<img width=\"500px\" src=\"img/pruning.png\" /> \n",
    "\n",
    "These *pruning* options are considered **hyperparameters**. Hyperparameters are parameters set prior to the model being trained and modulate how the model is trained and therefore directly influence the resulting parameters (weights) in the model.\n",
    "\n",
    "Below I define a decision tree with a maximum depth of `4` and a decision tree with a cost complexity pruning value of `0.01`, and for convenience I use the labels `neg` and `pos` instead of `0` and `1`. \n",
    "\n",
    "It also makes the tree much easier to read :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune the tree to a max depth of 4\n",
    "dtree_shallow = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "dtree_shallow = dtree_shallow.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize the pruned tree\n",
    "dot_data = \\\n",
    "    tree.export_graphviz(dtree_shallow, out_file=None, filled=True, \n",
    "                         feature_names=feature_cols, \n",
    "                         class_names=['neg','pos'])\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to run the \"pruned\" tree..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune the tree to using a complexity parameter of 0.01\n",
    "dtree_pruned = DecisionTreeClassifier(ccp_alpha=0.01) # default criterion=\"gini\"\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "dtree_pruned = dtree_pruned.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize the pruned tree\n",
    "dot_data = \\\n",
    "    tree.export_graphviz(dtree_pruned, out_file=None, filled=True, \n",
    "                         feature_names=feature_cols, \n",
    "                         class_names=['neg','pos'])\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating output\n",
    "We need to now evaluate the decision trees (pruned and full). For this, we will use the decision tree `predict()` method. The output of this method will be stored in the variables `y_pred_tree`, `y_pred_shallow`, and `y_pred_pruned`. These (predicted) variables will assedd using a confusion matrix.\n",
    "\n",
    "### Confusion matrix\n",
    "<img width=\"400px\" src=\"img/confusion_matrix3.png\" />\n",
    "\n",
    "A confusion matrix is a summary of prediction results on a classification problem.\n",
    "The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.\n",
    "The confusion matrix shows the ways in which your classification model is confused when it makes predictions.\n",
    "It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made. (https://www.geeksforgeeks.org/confusion-matrix-machine-learning/)\n",
    "\n",
    "**Definition of the Terms**:\n",
    "* Positive (P) : Observation is positive (for example: is an apple).\n",
    "* Negative (N) : Observation is not positive (for example: is not an apple).\n",
    "* True Positive (TP) : Observation is positive, and is predicted to be positive.\n",
    "* False Negative (FN) : Observation is positive, but is predicted negative.\n",
    "* True Negative (TN) : Observation is negative, and is predicted to be negative.\n",
    "* False Positive (FP) : Observation is negative, but is predicted positive.\n",
    "\n",
    "Using the confusion matrix, we can define the following metrics of evaluation.\n",
    "\n",
    "**Accuracy:**\n",
    "* (TP + TN) / (TP + TN + FP + FN)\n",
    "* Accuracy is the ratio of correct predictions to total predictions made. However, there are problems with accuracy. It assumes equal costs for both kinds of errors. A 99% accuracy can be excellent, good, mediocre, poor or terrible depending upon the problem.\n",
    "\n",
    "\n",
    "**Recall:**\n",
    "* TP / (TP + FN)\n",
    "* Recall is the ability of a classifier to find all positive instances. High recall indicates a small number of false negatives.\n",
    "\n",
    "\n",
    "**Precision:**\n",
    "* TP / (TP + FP)\n",
    "* Precision is the ability of a classifier not to label an instance positive that is actually negative. High precision indicates a small number of false positives.\n",
    "\n",
    "**F1 score (F measure):**\n",
    "* (2 * Recall * Precision) / (Recall + Precision)\n",
    "* Since we have two measures (Precision and Recall) it helps to have a measurement that represents both of them. We calculate an F1 score that uses Harmonic Mean in place of Arithmetic Mean as it punishes the extreme values more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to run the \"shallow\" tree..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict the response for dtree, dtree_shallow, and dtree_pruned decision trees\n",
    "y_pred_tree = dtree.predict(X_test)\n",
    "y_pred_shallow = dtree_shallow.predict(X_test)\n",
    "y_pred_pruned = dtree_pruned.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sci-kit learn also provides `accuracy_score`, `recall_score`, and `f1_score` functions to make these calculations easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tree:\", accuracy_score(y_test, y_pred_tree))\n",
    "print(\"shallow:\", accuracy_score(y_test, y_pred_shallow))\n",
    "print(\"pruned:\", accuracy_score(y_test, y_pred_pruned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the recall and precision scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tree:\", recall_score(y_test, y_pred_tree))\n",
    "print(\"shallow:\", recall_score(y_test, y_pred_shallow))\n",
    "print(\"pruned:\", recall_score(y_test, y_pred_pruned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"tree:\", precision_score(y_test, y_pred_tree))\n",
    "print(\"shallow:\", precision_score(y_test, y_pred_shallow))\n",
    "print(\"pruned:\", precision_score(y_test, y_pred_pruned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tree:\", f1_score(y_test, y_pred_tree))\n",
    "print(\"shallow:\", f1_score(y_test, y_pred_shallow))\n",
    "print(\"pruned:\", f1_score(y_test, y_pred_pruned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the accuracy and precision scores in the pruned and full trees are close. However the recall score is better in the pruned tree, meaning that the pruned tree has a smaller number of false negatives. Compare the lower left boxes in the two confusion matrices below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish, we can also visualize the confusion matrix of the trees. For convenience, I will define a function to do this. This will allow us to more easily visualize the confusion matrix later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(y_test, y_pred, palette=\"Set3\"):\n",
    "    ## see: https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "    ##      https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n",
    "    ##      https://classeval.wordpress.com/introduction/basic-evaluation-measures/\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    colors = sns.color_palette(palette) # set the colors to use for heatmap\n",
    "    # print(colors.as_hex()) # uncomment this to see color palette\n",
    "\n",
    "    ax = sns.heatmap(matrix, square=True, annot=True, fmt='d', \n",
    "                     cbar=False, cmap=colors, vmin=-1, annot_kws={\"size\":13}, linewidths=1.0)\n",
    "\n",
    "    # set labels on figure\n",
    "    ax.set_xticklabels(labels=[\"neg\",\"pos\"], fontsize=13)\n",
    "    ax.set_yticklabels(labels=[\"neg\",\"pos\"], fontsize= 13)\n",
    "    plt.xlabel(\"\\nactual value\", fontsize=15)\n",
    "    plt.ylabel(\"predicted value\\n\", fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "## show confusion matrix for dtree\n",
    "show_confusion_matrix(y_test, y_pred_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show confusion matrix for dtree_pruned\n",
    "show_confusion_matrix(y_test, y_pred_shallow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show confusion matrix for dtree_pruned\n",
    "show_confusion_matrix(y_test, y_pred_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, use one of the above confusion matrices to compute the recall and precision of either the full or pruned decision tree.\n",
    "* Recall: TP / (TP + FN)\n",
    "* Precision: TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "Random forest classifers are similar to decision trees in that they use hierarchical structures to split the dataset based on features. However, unlike decision trees, these classifiers use muliple decision trees (a \"forest\") in classification process using a method called *bagging*. Random forest is called an *ensemble* method because we have multiple classifiers by which we make our final prediction.\n",
    "\n",
    "The random forest algorithm consists of four general steps:\n",
    "* Select random samples from a given dataset - *bootstrapping*.\n",
    "* Construct a decision tree for each sample and get a prediction result from each decision tree.\n",
    "* Perform a vote for each predicted result.\n",
    "* Select the prediction result with the most votes as the final prediction - *aggregating*.\n",
    "\n",
    "<img width=\"500px\" src=\"img/random_forest_voting.png\" />\n",
    "\n",
    "**Advantages**\n",
    "* Random forests is considered as a highly accurate and robust method because of the number of decision trees participating in the process.\n",
    "* It does not suffer from the overfitting problem. The main reason is that it takes the average of all the predictions, which cancels out the biases.\n",
    "* The algorithm can be used in both classification and regression problems.\n",
    "* Random forests can also handle missing values. There are two ways to handle these: using median values to replace continuous variables, and computing the proximity-weighted average of missing values.\n",
    "* You can get the relative feature importance, which helps in selecting the most contributing features for the classifier.\n",
    "\n",
    "**Disadvantages**\n",
    "* Random forests is slow in generating predictions because it has multiple decision trees. Whenever it makes a prediction, all the trees in the forest have to make a prediction for the same given input and then perform voting on it. This whole process is time-consuming.\n",
    "* The model is difficult to interpret compared to a decision tree, where you can easily make a decision by following the path in the tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing random forest\n",
    "Like decision trees, building and fitting a random forest classifier is a straightforward task  in scikit-learn. First, we define a random forest classifier variable, and, second, we train the classifier by calling the `fit` method.\n",
    "\n",
    "Random forest has many hyperparameters. Hyperparameters included in Random Forest are:\n",
    "* `n_estimators` = number of trees in the forest\n",
    "* `criterion` = the criterion used to choose a split at each node (e.g. gini, entropy, mse, etc.)\n",
    "* `max_depth` = maximum length of the longest route in each tree\n",
    "* `min_samples_split` = minimum number of samples to split on at a node\n",
    "* `max_leaf_nodes` = maximum number of leaf nodes\n",
    "* `max_features` = maximum number of random features to test at each node\n",
    "* `max_samples` = size of bootstrapped dataset for each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build and fit random forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating random forest\n",
    "We can evaluate the our random forest classifier by calculating the accuracy, recall, precision, and F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_forest = rfc.predict(X_test)\n",
    "y_proba_forest = list(zip(*rfc.predict_proba(X_test)))[1]\n",
    "accuracy_score(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can display the `confusion_matrix` of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get values for confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_forest).ravel()\n",
    "print((tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## show confusion matrix for random forest\n",
    "show_confusion_matrix(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_static_roc_curve(fpr, tpr):\n",
    "    plt.figure(figsize=[5,5])\n",
    "    plt.fill_between(fpr, tpr, alpha=.5, color='darkorange')\n",
    "    # Add dashed line with a slope of 1\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2)\n",
    "    plt.plot([0,1], [0,1], linestyle=(0, (5, 5)), linewidth=2)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC curve\");\n",
    "    \n",
    "def plot_static_pr_curve(recall, precision):\n",
    "    plt.figure(figsize=[5,5])\n",
    "    plt.fill_between(recall, precision, alpha=.5, color='darkorange')\n",
    "    plt.plot(recall, precision, color='darkorange', lw=2)\n",
    "    # Add dashed line with a slope of 1\n",
    "    plt.plot([1,0], [0,1], linestyle=(0, (5, 5)), linewidth=2)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-recall curve\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_proba_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_forest)\n",
    "plot_static_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba_forest)\n",
    "auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_static_pr_curve(recall,precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Cross-validation is key to choosing the best possible hyperparameters. This involves splitting the training set into $k$ number of subsets where one subset is used as a validation set and the remaining $k-1$ are used for training. This is then completed over all possible sets of $k$ and the average of the metrics is used to assess the model with the given hyperparameters.\n",
    "\n",
    "To further this idea, we can use cross-validation in concert with a *grid search* which runs a model with variable hyperparameters that are defined by lists of values. This will \"check\" the metrics for each of this runs and average them. The optimal combination of hyperparameters will be outputted as the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees to be used\n",
    "rfc_n_estimators = [int(x) for x in np.linspace(100, 500, 5)]\n",
    "# Maximum length in tree\n",
    "rfc_max_depth = [int(x) for x in np.linspace(2, 10, 5)]\n",
    "\n",
    "rfc_grid = {'n_estimators': rfc_n_estimators,\n",
    "            'max_depth': rfc_max_depth}\n",
    "\n",
    "# Create the model to be tuned\n",
    "rfc_base = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the random search Random Forest\n",
    "rfc_random = RandomizedSearchCV(estimator = rfc_base, param_distributions = rfc_grid, \n",
    "                                n_iter = 200, cv = 4, scoring='f1',\n",
    "                                random_state = 42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rfc_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal parameters\n",
    "rfc_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best = rfc_random.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature ranking\n",
    "In addition to evaluating the random forest classifier, it is sometimes helpful to see how important each of the features were in arriving at final predictions. If we notice that a feature is of little importance, we can eliminate it from our training dataset in order to gain efficiency.\n",
    "\n",
    "When building a random forest classifier, scikit-learn returns a variable named `feature_importances_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find important features\n",
    "rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw output is a little difficult to interpret. So, we will put the output in a Pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = \\\n",
    "    pds.Series(rfc.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the feature importances using a seaborn barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize important features\n",
    "%matplotlib inline\n",
    "\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "\n",
    "# Add labels to your graph\n",
    "plt.xlabel('\\nFeature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\\n\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "How does this differ from Random Forest? Random Forest uses bagging in order to train a final model. XGBoost works by a method called **boosting**, which is an iterative, sequential method that adds a new decision tree to the overall model at each step to minimize error from the previous trees. Each new tree is a *weak learner* that when all combined creates a strong learner that will accurately predict the outcome.\n",
    "\n",
    "<img width=\"500px\" src=\"img/xgboost_boosting.png\" />\n",
    "\n",
    "A problem with XGBoost is that it is highly sensitive to it's hyperparameters. If too many trees are added, it can be overfit. Moreover, the `learning rate` is crucial because the model will perform better if trained slowly, but the likelihood of many trees being created increases with a decreaed learning rate. FInding the right balance for the model is key to the robustness and generalizability of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## build and fit XGBoost classifier\n",
    "xgc = xgb.XGBClassifier(objective='reg:logistic',n_estimators=100, \\\n",
    "                        alpha=0.01, max_depth=4, learning_rate=0.1, \\\n",
    "                        colsample_bytree=0.3, use_label_encoder=False)\n",
    "xgc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_boost = xgc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_test, y_pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_imp = \\\n",
    "    pds.Series(xgc.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "\n",
    "## visualize important features\n",
    "%matplotlib inline\n",
    "\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "\n",
    "# Add labels to your graph\n",
    "plt.xlabel('\\nFeature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\\n\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "In this module, we have covered a numbered of important concepts related to decision tree and random forest classifiers, such as:\n",
    "* Inspecting the data using correlation matrices and histograms\n",
    "* Building training and test datasets\n",
    "* Defining decision tree and random forest classifiers using scikit-learn and predicting target classes\n",
    "* Evaluating classifiers using a confusion matrix and calculating the accuracy, precision, recall, and F1 scores.\n",
    "* Inspecting the importance of features in our classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion topics\n",
    " \n",
    "* Interpretability of different models\n",
    "* Handling high dimensionality of biomedical data\n",
    "* Models that account for longitudinal data for patient populations\n",
    "* Missingness in datasets -- imputation and omission\n",
    "* Generalizability of models for biomedical applications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and additional reading\n",
    "\n",
    "In this module, we covered the basics of implementing and evaluating a logistic regression classifier in scikit learns and a neural network using keras. \n",
    "\n",
    "* Burkov A. The Hundred-Page Machine Learning Book by Andriy Burkov. Expert Systems. 2019;5(2):132-50.\n",
    "* Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas J. Scikit-learn: Machine learning in Python. the Journal of machine Learning research. 2011 Nov 1;12:2825-30.\n",
    "* Chollet F. Keras documentation. keras.io. 2015;33.\n",
    "* Goodfellow I, Bengio Y, Courville A. Deep learning.\n",
    "* Bishop C. Pattern Recognition and Machine Learning.\n",
    "* Friedman JH, Tibshirani R, Hastie T. The Elements of Statistical Learning.\n",
    "* https://www.codecademy.com/learn/machine-learning\n",
    "* https://www.w3schools.com/python/python_ml_getting_started.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Portion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 01 (8 points)\n",
    "\n",
    "Create your own function that calculates the sensitivity and specificity for a given confusion matrix. The inputs should be true positives, false postiives, true negatives, and false negatives. The outputs should be sensivity and specificity. This function should print the calculated sensitivity and specicity as well as return both values.\n",
    "\n",
    "Name the function sens_and_spec and have the inputs as positional arugments in the order: tp, fp, tn, fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "sens_and_spec(300,30,250,70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 02 (8 points)\n",
    "\n",
    "Create your own function that calculates the precision and recall for a given confusion matrix. The inputs should be true positives, false postiives, true negatives, and false negatives. The outputs should be precision and recall. This function should print the calculated precision and recall as well as return both values.\n",
    "\n",
    "Name the function prec_and_recall and have the inputs as positional arugments in the order: tp, fp, tn, fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "prec_and_recall(300,30,250,70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 03 (4 points)\n",
    "\n",
    "Create your own function to calculate the F1 score for a given precision and recall. The inputs should be preicions and recall. The output should be F1 score. This function should print the calculated F1 score as well as return the f1 score value.\n",
    "\n",
    "Name the function f1_score and have the inputs as positional arugments in the order: precision, recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "f1_score(0.909,0.811)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
